{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from recording import Recording\n",
    "import utils as utl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "# global settings\n",
    "params = {\n",
    "    # bin size [s]\n",
    "    'bin_size'        : 0.2,\n",
    "\n",
    "    # filter units\n",
    "    # thresholds for source (src) and target (trg) population\n",
    "    # note: `None`: no upper/lower limit\n",
    "    # note: for within-region analysis, only source thresholds apply\n",
    "    'rate_src': (1, None),            # firing rate [Hz] interval\n",
    "    'rate_trg': (1, None),\n",
    "    'spike_width_src': (None, None), # spike width [ms] interval\n",
    "    'spike_width_trg': (  .5, None),\n",
    "    'perc_trials': 0.9,              # percentage of trials to be covered\n",
    "                                     # note: Since the valid trial range for neurons may differ greatly, we choose a percentage of trials\n",
    "                                     #       that we want to keep (0.9 = 90 %). Then, we drop neurons until the remaining neurons\n",
    "                                     #       cover at least this part of the maximum trial range.\n",
    "\n",
    "    # filter trials\n",
    "    'first_lick' : (None, None), # lick time [s] interval, relative to cue\n",
    "    'type_incl': [ 'l_n', ],   # trial types (list of strings)\n",
    "                               # note: Define strings to match beginning of `unit(1).Behavior.stim_type_name` strings.\n",
    "                               #       To include all trials, set list empty\n",
    "\n",
    "\n",
    "    # score reported in the output\n",
    "    # note: The definition of the score does not affect definition of the loss function\n",
    "    #       and therefore does not affect the model parameters.\n",
    "    # note: see https://scikit-learn.org/stable/modules/model_evaluation.html for available scorers\n",
    "    'scoring': 'r2',\n",
    "\n",
    "    # subtract baseline \n",
    "    # note: This subtracts the average firing rate during pre cue period per trial\n",
    "    'subtract_baseline': True\n",
    "\n",
    "}\n",
    "\n",
    "# define trial epochs \n",
    "# epoch_name : (start, end, alignment)\n",
    "# note:\n",
    "#   (i): Epochs define the half-open interval [start, end)\n",
    "#  (ii): Lick times may fall between bins, because bins are aligned to cue\n",
    "# (iii): Default epoch ('all') defined as ['cue' - 2 s, 'lick' + 2 s)\n",
    "#        Only complete bins are kept. For example, if bin_size = 0.2 and first_lick = 1.81,\n",
    "#        then the last bin in this trial is defined as [1.98, 2.00)\n",
    "epochs = {\n",
    "    'all'       : (None, None, 'cue'), # None only works with 'cue' alignment\n",
    "    'pre_cue'   : (-.6,  .0,   'cue'),\n",
    "    'post_cue1' : ( .0,  .6,   'cue'),\n",
    "    'post_cue2' : ( .6, 1.2,   'cue'),\n",
    "    'pre_lick'  : (-.6,  .0,  'lick'),\n",
    "    'post_lick' : ( .0,  .6,  'lick'),\n",
    "}\n",
    "\n",
    "# define sets of trials \n",
    "trial_groups = {\n",
    "    'lick_0.6': (0.6, None, 'lick'), # e.g. all trials with lick times relative to cue > 0.6 s\n",
    "    'lick_1.2': (1.2, None, 'lick'),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select units: single region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choose recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # single ALM probe\n",
    "# rec1 = Recording('./data/zidan/ALM_ALM/MK22_20230301/MK22_20230301_2H2_g0_JRC_units_probe1.mat')\n",
    "# rec2 = None\n",
    "\n",
    "# # ALM-ALM\n",
    "# rec1 = Recording('./data/zidan/ALM_ALM/MK22_20230301/MK22_20230301_2H2_g0_JRC_units_probe2.mat')\n",
    "# rec2 = Recording('./data/zidan/ALM_ALM/MK22_20230301/MK22_20230301_2H2_g0_JRC_units_probe1.mat')\n",
    "\n",
    "# ALM-Str (imec0: STR)\n",
    "rec2 = Recording('./data/zidan/ALM_STR/ZY78_20211015/ZY78_20211015NP_g0_JRC_units.mat')\n",
    "rec1 = Recording('./data/zidan/ALM_STR/ZY78_20211015/ZY78_20211015NP_g0_imec0_JRC_units.mat')\n",
    "\n",
    "# # ALM-Thal (imec0: Thal)\n",
    "# rec2 = Recording('./data/zidan/ALM_Thal/ZY113_20220617/ZY113_20220617_NPH2_g0_JRC_units.mat')\n",
    "# rec1 = Recording('./data/zidan/ALM_Thal/ZY113_20220617/ZY113_20220617_NPH2_g0_imec0_JRC_units.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select units and trials, and bin data\n",
    "dfx_bin, dfy_bin = utl.select_data(rec1, rec2=rec2, params=params)\n",
    "\n",
    "# subtract baseline\n",
    "dfx_bin0 = utl.subtract_baseline(dfx_bin, rec1.df_spk)\n",
    "dfy_bin0 = utl.subtract_baseline(dfy_bin, rec1.df_spk if rec2 is None else rec2.df_spk)\n",
    "\n",
    "# optional: filter some epoch\n",
    "dfx_bin0_epo = utl.select_epoch(dfx_bin0, epochs['pre_lick'], rec1.df_trl)\n",
    "dfy_bin0_epo = utl.select_epoch(dfy_bin0, epochs['pre_lick'], rec1.df_trl if rec2 is None else rec2.df_trl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression (= ridge with alpha=0)\n",
    "lin_mods = utl.ridge_regression(dfx_bin0, dfy_bin0, scoring=params['scoring'], alphas=[0])\n",
    "lin_mod = lin_mods.best_estimator_\n",
    "\n",
    "# ridge\n",
    "ridge_mods = utl.ridge_regression(dfx_bin0, dfy_bin0, scoring=params['scoring'], alphas=np.logspace(-13, 13, 27))\n",
    "ridge_mod = ridge_mods.best_estimator_\n",
    "utl.plot_gridsearch(ridge_mods, 'ridge', other_mods={'linear': lin_mods}, logscale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RRR\n",
    "rr_mods = utl.reduced_rank_regression(dfx_bin0, dfy_bin0, scoring=params['scoring'])\n",
    "rr_mod = rr_mods.best_estimator_\n",
    "utl.plot_gridsearch(rr_mods, 'reduced rank', other_mods={'linear': lin_mods, 'ridge': ridge_mods}, logscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "Y_pred, scores = utl.get_ypred(dfx_bin0, dfy_bin0, ridge_mod, scoring=params['scoring'])\n",
    "utl.plot_mean_response(dfy_bin0, Y_pred, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remote batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch process all recordings\n",
    "p_root = Path(r'X:\\Users\\Zidan\\DataForNico\\data')\n",
    "\n",
    "if not p_root.is_dir():\n",
    "    print('Invalid rood dir. No access to network drive?')\n",
    "\n",
    "else: \n",
    "    p_dirs = \\\n",
    "        [ *p_root.glob('ALM_ALM/*/') ] + \\\n",
    "        [ *p_root.glob('ALM_STR/*/') ] + \\\n",
    "        [ *p_root.glob('ALM_Thal/*/') ]\n",
    "\n",
    "name2region = {\n",
    "    'MK22_20230301_2H2_g0_JRC_units_probe1.mat' : 'ALM1',\n",
    "    'MK22_20230301_2H2_g0_JRC_units_probe2.mat' : 'ALM2',\n",
    "    'MK22_20230303_2H2_g0_JRC_units_probe1.mat' : 'ALM1',\n",
    "    'MK22_20230303_2H2_g0_JRC_units_probe2.mat' : 'ALM2',\n",
    "    'MK25_20230314_2H2_g0_JRC_units_probe1.mat' : 'ALM1',\n",
    "    'MK25_20230314_2H2_g0_JRC_units_probe2.mat' : 'ALM2',\n",
    "    'ZY78_20211015NP_g0_imec0_JRC_units.mat'    : 'STR',\n",
    "    'ZY78_20211015NP_g0_JRC_units.mat'          : 'ALM',\n",
    "    'ZY82_20211028NP_g0_imec0_JRC_units.mat'    : 'STR',\n",
    "    'ZY82_20211028NP_g0_JRC_units.mat'          : 'ALM',\n",
    "    'ZY83_20211108NP_g0_imec0_JRC_units.mat'    : 'STR',\n",
    "    'ZY83_20211108NP_g0_JRC_units.mat'          : 'ALM',\n",
    "    'ZY113_20220617_NPH2_g0_imec0_JRC_units.mat': 'THA',\n",
    "    'ZY113_20220617_NPH2_g0_JRC_units.mat'      : 'ALM',\n",
    "    'ZY113_20220618_NPH2_g0_imec0_JRC_units.mat': 'THA',\n",
    "    'ZY113_20220618_NPH2_g0_JRC_units.mat'      : 'ALM',\n",
    "    'ZY113_20220620_NPH2_g0_imec0_JRC_units.mat': 'THA',\n",
    "    'ZY113_20220620_NPH2_g0_JRC_units.mat'      : 'ALM',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precalculate bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bin_size in [.2, 1e-3]:\n",
    "    print(bin_size)\n",
    "    for p_mat in p_root.glob('**/*mat'):\n",
    "\n",
    "        print(p_mat)\n",
    "        rec = Recording(p_mat)\n",
    "\n",
    "        rec.path_bin = rec._path_tmp / 'bin{}.hdf'.format(bin_size)\n",
    "        df_bin = rec._assign_df(rec.path_bin, utl.bin_spikes, {'df_spk': rec.df_spk, 'df_trl': rec.df_trl, 'bin_size': bin_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bin_size = 1e-3\n",
    "for p_mat in p_root.glob('**/*mat'):\n",
    "\n",
    "    print(p_mat)\n",
    "    rec = Recording(p_mat)\n",
    "\n",
    "    rec.path_bin = rec._path_tmp / 'bin{}.hdf'.format(bin_size)\n",
    "    df_bin = rec._assign_df(rec.path_bin, utl.bin_spikes, {'df_spk': rec.df_spk, 'df_trl': rec.df_trl, 'bin_size': bin_size})\n",
    "    \n",
    "    df_bin = df_bin.apply(uniform_filter1d, axis=0, size=50)\n",
    "    # utl.plot_mean_response(df_bin, path=rec._path_tmp / 'psth1ms.png')\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_wrapper(p_out, params, recX, recY):\n",
    "\n",
    "    # create folder\n",
    "    p_out.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # path for params.json\n",
    "    p_json = p_out / 'params.json'\n",
    "    if p_json.exists():\n",
    "        print(f'params.json found. Skipping {p_out}')\n",
    "        return \n",
    "    \n",
    "    # load data, select trials and units based on `params`\n",
    "    dfx_bin, dfy_bin = utl.select_data(recX, rec2=recY, params=params)\n",
    "\n",
    "    # subtract baseline, if required\n",
    "    if params['subtract_baseline']:\n",
    "        dfx_bin = utl.subtract_baseline(dfx_bin, recX.df_spk)\n",
    "        dfy_bin = utl.subtract_baseline(dfy_bin, recX.df_spk if recY is None else recY.df_spk)\n",
    "    if dfx_bin.empty:\n",
    "        print(f'INFO no data left, skipping recX: {recX.session}, recY: {recY.session}')\n",
    "        return\n",
    "\n",
    "    # do fit for each epoch separately\n",
    "    for name, epo in epochs.items():\n",
    "\n",
    "        # output folder for epoch\n",
    "        p_out_epo = p_out / name\n",
    "        p_out_epo.mkdir(exist_ok=True)\n",
    "\n",
    "        # select subset of data\n",
    "        dfx_bin_epo = utl.select_epoch(dfx_bin, epo, recX.df_trl)\n",
    "        dfy_bin_epo = utl.select_epoch(dfy_bin, epo, recX.df_trl if recY is None else recY.df_trl)\n",
    "\n",
    "        if dfx_bin_epo.empty:\n",
    "            print(f'INFO no data left in epoch {name}, skipping recX: {recX.session}, recY: {recY.session}')\n",
    "            continue\n",
    "\n",
    "        # linear regression (= ridge with alpha=0)\n",
    "        lin_mods = utl.ridge_regression(dfx_bin_epo, dfy_bin_epo, scoring=params['scoring'], alphas=[0])\n",
    "        utl.save_cv_results(lin_mods, path=p_out_epo / 'reg_linear.parquet')\n",
    "\n",
    "        # ridge regression\n",
    "        ridge_mods = utl.ridge_regression(dfx_bin_epo, dfy_bin_epo, scoring=params['scoring'], alphas=np.logspace(-13, 13, 27))\n",
    "        ridge_mod = ridge_mods.best_estimator_\n",
    "        utl.save_cv_results(ridge_mods, path=p_out_epo / 'reg_ridge.parquet')\n",
    "\n",
    "        # RRR\n",
    "        rr_mods = utl.reduced_rank_regression(dfx_bin_epo, dfy_bin_epo, scoring=params['scoring'])\n",
    "        utl.save_cv_results(rr_mods, path=p_out_epo / 'reg_rrr.parquet')\n",
    "\n",
    "        # plot regressions\n",
    "        utl.plot_gridsearch(ridge_mods, 'ridge', other_mods={'linear': lin_mods}, logscale=True, path=p_out_epo / 'reg_ridge.png')\n",
    "        utl.plot_gridsearch(rr_mods, 'reduced rank', other_mods={'linear': lin_mods, 'ridge': ridge_mods}, logscale=False, path=p_out_epo / 'reg_rrr.png')\n",
    "\n",
    "        # prediction\n",
    "        Y_pred, scores = utl.get_ypred(dfx_bin_epo, dfy_bin_epo, ridge_mod, scoring=params['scoring'])\n",
    "        utl.plot_mean_response(dfy_bin_epo, Y_pred, scores, path=p_out_epo / 'pred_ridge.png')\n",
    "        ds = pd.Series(scores, name=params['scoring'])\n",
    "        ds.index.name = 'unit'\n",
    "        ds.to_csv(p_out_epo / 'pred_ridge_scores.csv', index=True)\n",
    "\n",
    "    # save params\n",
    "    pd.Series(params).to_json(p_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_params = {\n",
    "    'all_depth_0.2': {\n",
    "        'bin_size'          : 0.2, \n",
    "        'spike_width_src'   : (None, None),\n",
    "        'spike_width_trg'   : (  .5, None),\n",
    "        'first_lick'        : (None, None), \n",
    "        'type_incl'         : [ 'l_n', ],       \n",
    "        'rate_src'          : (1, None),\n",
    "        'rate_trg'          : (1, None),        \n",
    "        'perc_trials'       : 0.9,  \n",
    "        'scoring'           : 'r2', \n",
    "        'subtract_baseline' :  True\n",
    "        },\n",
    "    'all_depth_0.2_sw': {\n",
    "        'bin_size'          : 0.2, \n",
    "        'spike_width_src'   : (None, None),\n",
    "        'spike_width_trg'   : (None, .5), \n",
    "        'first_lick'        : (None, None), \n",
    "        'type_incl'         : [ 'l_n', ],       \n",
    "        'rate_src'          : (1, None),\n",
    "        'rate_trg'          : (1, None),        \n",
    "        'perc_trials'       : 0.9,  \n",
    "        'scoring'           : 'r2', \n",
    "        'subtract_baseline' :  True\n",
    "        },\n",
    "    'all_depth_0.2_lck0.6': {\n",
    "        'bin_size'          : 0.2, \n",
    "        'spike_width_src'   : (None, None),\n",
    "        'spike_width_trg'   : (.5, None), \n",
    "        'first_lick'        : (None, .6), \n",
    "        'type_incl'         : [ 'l_n', ],       \n",
    "        'rate_src'          : (1, None),\n",
    "        'rate_trg'          : (1, None),        \n",
    "        'perc_trials'       : 0.9,  \n",
    "        'scoring'           : 'r2', \n",
    "        'subtract_baseline' :  True\n",
    "        },\n",
    "    'all_depth_0.2_lck1.2': {\n",
    "        'bin_size'          : 0.2, \n",
    "        'spike_width_src'   : (None, None),\n",
    "        'spike_width_trg'   : (.5, None), \n",
    "        'first_lick'        : (None, 1.2), \n",
    "        'type_incl'         : [ 'l_n', ],       \n",
    "        'rate_src'          : (1, None),\n",
    "        'rate_trg'          : (1, None),        \n",
    "        'perc_trials'       : 0.9,  \n",
    "        'scoring'           : 'r2', \n",
    "        'subtract_baseline' :  True\n",
    "        },\n",
    "    'all_depth_0.2_lck1.8': {\n",
    "        'bin_size'          : 0.2, \n",
    "        'spike_width_src'   : (None, None),\n",
    "        'spike_width_trg'   : (.5, None), \n",
    "        'first_lick'        : (None, 1.8), \n",
    "        'type_incl'         : [ 'l_n', ],       \n",
    "        'rate_src'          : (1, None),\n",
    "        'rate_trg'          : (1, None),        \n",
    "        'perc_trials'       : 0.9,  \n",
    "        'scoring'           : 'r2', \n",
    "        'subtract_baseline' :  True\n",
    "        },\n",
    "    'all_depth_raw_0.2': {\n",
    "        'bin_size'          : 0.2, \n",
    "        'spike_width_src'   : (None, None),\n",
    "        'spike_width_trg'   : (  .5, None), \n",
    "        'first_lick'        : (None, None), \n",
    "        'type_incl'         : [ 'l_n', ],       \n",
    "        'rate_src'          : (1, None),\n",
    "        'rate_trg'          : (1, None),        \n",
    "        'perc_trials'       : 0.9,  \n",
    "        'scoring'           : 'r2', \n",
    "        'subtract_baseline' :  False\n",
    "        },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, params in d_params.items():\n",
    "    print(f'>>>> starting parameter set {name}....')\n",
    "\n",
    "    for p_dir in p_dirs:\n",
    "\n",
    "        print(p_dir)\n",
    "\n",
    "        # load recordings\n",
    "        p_matA, p_matB = [ *p_dir.glob('*.mat')]\n",
    "        recA, recB = Recording(p_matA, tmp_dir='analysis/tmp'), Recording(p_matB, tmp_dir='analysis/tmp')\n",
    "        regA, regB = name2region[p_matA.name], name2region[p_matB.name]\n",
    "\n",
    "        # regA -> regB\n",
    "        p_out = p_dir / f'analysis/{name}/{regA}_{regB}'\n",
    "        proc_wrapper(p_out, params, recA, recB)\n",
    "\n",
    "        # regB -> regA\n",
    "        p_out = p_dir / f'analysis/{name}/{regB}_{regA}'\n",
    "        proc_wrapper(p_out, params, recB, recA)\n",
    "\n",
    "        # regA\n",
    "        p_out = p_dir / f'analysis/{name}/{regA}'\n",
    "        proc_wrapper(p_out, params, recA, None)\n",
    "\n",
    "        # regB\n",
    "        p_out = p_dir / f'analysis/{name}/{regB}'\n",
    "        proc_wrapper(p_out, params, recB, None)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scores(ps_csv):\n",
    "\n",
    "    dfs = []\n",
    "    for p_csv in ps_csv:\n",
    "\n",
    "        parts = p_csv.parts\n",
    "        epoch = parts[-2]\n",
    "        inter = parts[-3]\n",
    "        setti = parts[-4]\n",
    "        recor = parts[-6]\n",
    "        anima, date = recor.split('_')\n",
    "        probe = parts[-7]\n",
    "\n",
    "        df = pd.read_csv(p_csv)\n",
    "\n",
    "        dfs.append(pd.DataFrame(data={\n",
    "            'unit':         df.loc[:, 'unit'], # TODO change this for newer data\n",
    "            'score':        df.iloc[:, 1],\n",
    "            'epoch':        epoch,\n",
    "            'interaction':  inter,\n",
    "            'settings':     setti,\n",
    "            'recording':    recor,\n",
    "            'animal':       anima,\n",
    "            'date' :        date,\n",
    "            'probes':       probe\n",
    "        })\n",
    "        )\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df.loc[:, 'interaction_'] = df.loc[:, 'interaction'].map(lambda x: x.replace('ALM1', 'ALM').replace('ALM2', 'ALM'))\n",
    "    df.loc[:, 'n_regions'] = df.loc[:, 'interaction'].apply(lambda x: len(x.split('_')))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dfs = []\n",
    "for name in d_params.keys():\n",
    "    \n",
    "    print(f'INFO: Now processing {name}')\n",
    "    p_plot = p_root / f'plots/{name}'\n",
    "    p_plot.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    p_csvs = [ *p_root.glob(f'**/{name}/**/pred_ridge_scores.csv') ]\n",
    "    if not p_csvs:\n",
    "        print(f'No CSV files found: skipping {name}')\n",
    "        continue\n",
    "    \n",
    "    df = load_scores(p_csvs)\n",
    "    gr_epo = df.groupby('epoch')\n",
    "    df_all = gr_epo.get_group('all')\n",
    "    \n",
    "    for epo, df_epo in gr_epo:\n",
    "\n",
    "        \n",
    "        # g = sns.catplot(df_epo, x='interaction', y='score', col='probes', hue='recording', sharex=False, facet_kws={'ylim': (-1, 1)}, dodge=True)\n",
    "        # g.fig.savefig(p_plot / f'scores_{epo}.png')\n",
    "        # plt.close(g.fig)\n",
    "\n",
    "        # df_epo.loc[:, '_sort'] = df_epo.loc[:, 'interaction_'].str.len()\n",
    "        # d = df_epo.sort_values(by=['_sort', 'interaction_'])\n",
    "        # fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        # sns.stripplot(data=d, ax=ax, x='interaction_', y='score', hue='recording', dodge=True)\n",
    "        # ax.set_ylim((-1, 1))\n",
    "        # fig.savefig(p_plot / f'pooled_scores_{epo}.png')\n",
    "        # plt.close(fig)\n",
    "\n",
    "        df.loc[df_epo.index, 'dscore'] = df_epo.loc[:, 'score'].values - df_all.loc[:, 'score'].values\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = {\n",
    "    'all_depth_0.2': '> .5ms',\n",
    "}\n",
    "\n",
    "m = df.loc[:, 'settings'].isin(sel.keys())\n",
    "\n",
    "g = sns.catplot(df, \n",
    "            x='interaction', y='score', col='probes',  hue='epoch',\n",
    "            kind='box', whis=0, fliersize=0, palette='pastel', hue_order=epochs.keys(),\n",
    "            sharex=False, facet_kws={'ylim': (-.25, .85)}) #, dodge=True) #, aspect=1.2)\n",
    "g.map(sns.stripplot, 'interaction', 'score', 'epoch',\n",
    " hue_order=epochs.keys(), dodge=True, palette='deep', \n",
    " edgecolor='auto', linewidth=.5, size=1)\n",
    "\n",
    "g.fig.savefig('epoch.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = {\n",
    "    'all_depth_0.2': '> .5ms',\n",
    "}\n",
    "\n",
    "m = df.loc[:, 'settings'].isin(sel.keys())\n",
    "\n",
    "g = sns.catplot(df, \n",
    "            x='interaction', y='dscore', col='probes',  hue='epoch',\n",
    "            kind='box', whis=0, fliersize=0, palette='pastel', hue_order=epochs.keys(),\n",
    "            sharex=False, facet_kws={'ylim': (-1, 1)}) #, dodge=True) #, aspect=1.2)\n",
    "g.map(sns.stripplot, 'interaction', 'dscore', 'epoch',\n",
    " hue_order=epochs.keys(), dodge=True, palette='deep', \n",
    " edgecolor='auto', linewidth=.5, size=1)\n",
    "\n",
    "g.fig.savefig('d_epoch.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = {\n",
    "    'all_depth_0.2': '> .5ms',\n",
    "    'all_depth_0.2_sw': '< .5 ms' \n",
    "}\n",
    "label = 'spike width'\n",
    "\n",
    "m = df.loc[:, 'settings'].isin(sel.keys())\n",
    "df.loc[:, label] = df.loc[:, 'settings'].map(sel)\n",
    "\n",
    "g = sns.catplot(df.loc[m, :], \n",
    "            x='interaction', y='score', col='probes',  hue=label,\n",
    "            kind='box', whis=0, fliersize=0, palette='pastel', hue_order=sel.values(),\n",
    "            sharex=False, facet_kws={'ylim': (-.25, .85)}, dodge=True) #, aspect=1.2)\n",
    "g.map(sns.stripplot, 'interaction', 'score', label,\n",
    "  hue_order=sel.values(), dodge=True, palette='deep', \n",
    " edgecolor='auto', linewidth=.5, size=2)\n",
    "\n",
    "g.fig.savefig('sw.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = {\n",
    "    'all_depth_0.2': 'True',\n",
    "    'all_depth_raw_0.2': 'False' \n",
    "}\n",
    "label = 'baseline subtracted'\n",
    "\n",
    "m = df.loc[:, 'settings'].isin(sel.keys())\n",
    "df.loc[:, label] = df.loc[:, 'settings'].map(sel)\n",
    "\n",
    "g = sns.catplot(df.loc[m, :], \n",
    "            x='interaction', y='score', col='probes',  hue=label,\n",
    "            kind='box', whis=0, fliersize=0, palette='pastel', hue_order=df.loc[m, label].unique(),\n",
    "            sharex=False, facet_kws={'ylim': (-.25, .85)}, dodge=True) #, aspect=1.2)\n",
    "g.map(sns.stripplot, 'interaction', 'score', label,\n",
    "  hue_order=df.loc[m, label].unique(), dodge=True, palette='deep', \n",
    " edgecolor='auto', linewidth=.5, size=2)\n",
    "\n",
    "g.fig.savefig('baseline.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = {\n",
    "    'all_depth_0.2_lck0.6': '< 0.6 s',\n",
    "    'all_depth_0.2_lck1.2': '< 1.2 s',\n",
    "    'all_depth_0.2_lck1.8': '< 1.8 s',\n",
    "    'all_depth_0.2': 'all' \n",
    "}\n",
    "label = 'lick time'\n",
    "\n",
    "m = df.loc[:, 'settings'].isin(sel.keys())\n",
    "df.loc[:, label] = df.loc[:, 'settings'].map(sel)\n",
    "\n",
    "g = sns.catplot(df.loc[m, :], \n",
    "            x='interaction', y='score', col='probes',  hue=label,\n",
    "            kind='box', whis=0, fliersize=0, palette='pastel', hue_order=sel.values(),\n",
    "            sharex=False, facet_kws={'ylim': (-.25, .85)}, dodge=True) #, aspect=1.2)\n",
    "g.map(sns.stripplot, 'interaction', 'score', label,\n",
    "  hue_order=sel.values(), dodge=True, palette='deep', \n",
    " edgecolor='auto', linewidth=.5, size=2)\n",
    "\n",
    "g.fig.savefig('lick_time.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "for ax, (k, d) in zip(axarr, df.groupby('n_regions')):\n",
    "    sns.histplot(ax=ax, data=d, x='score', hue='interaction_', stat='density', common_norm=False, bins=np.linspace(-1, 1, 40), kde=True)\n",
    "    ax.set_xlim(-.25, 1)\n",
    "\n",
    "fig.savefig('scores_hist.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=df, \n",
    "                x='interaction', y='score', col='recording', col_wrap=3, hue='bin_size',\n",
    "                sharex=False, facet_kws={'ylim': (-1, 1)},\n",
    "                kind='box', whis=0, fliersize=0, palette='pastel',\n",
    "                dodge=True, height=4, aspect=1.2)\n",
    "g.map(sns.stripplot, 'interaction', 'score', 'bin_size', dodge=True, palette='deep', edgecolor='auto', linewidth=.5)\n",
    "# g.fig.savefig(p_plot / f'scores_{name}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(df, \n",
    "            x='interaction', y='dscore', row='probes', hue='epoch', hue_order=epochs.keys(),\n",
    "            # kind='box', whis=0, fliersize=0, palette='pastel',\n",
    "            sharex=False, facet_kws={'ylim': (-1, 1)}, dodge=True, palette='deep')\n",
    "# g.map(sns.stripplot, 'interaction', 'dscore', 'epoch', hue_order=epochs.keys(), dodge=True, palette='deep', edgecolor='auto', linewidth=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axmat = plt.subplots(nrows=3, ncols=4, figsize=(20, 15))\n",
    "legend = True\n",
    "for axarr, (pro, df_pro) in zip(axmat, df.groupby('probes')):\n",
    "    print(pro, len(axarr))\n",
    "    for ax, (inter, df_inter) in zip(axarr, df_pro.groupby('interaction')):\n",
    "\n",
    "        sns.kdeplot(ax=ax, data=df_inter, x='score', hue='epoch', legend=legend)\n",
    "        legend = False\n",
    "\n",
    "        ax.set_xlim(-1, 1)\n",
    "        ax.set_title(f'{pro}: {inter}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
